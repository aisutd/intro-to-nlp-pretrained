{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Use CBOW Model to generate word embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2TJYJREwUAo"
      },
      "source": [
        "# Prepare training data for Word2Vec\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PHx2miSexQg"
      },
      "source": [
        "## Download text corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuw6Pd8LEUFS"
      },
      "source": [
        "#### we can explain Wget is a command that can retrieve files, use is like:\"wget url -O output_name\" \r\n",
        "import urllib.request\r\n",
        "## Aim: prepare corpus file, download using url to your local storage, Wget can retrieve files from various web servers\r\n",
        "\r\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/aisutd/Intro-to-NLP-pretrained/main/word_embedding_models/data/sample_corpus_essay_Bach_Nguyen.txt -O ./sample_corpus.txt\r\n",
        "\r\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/aisutd/Intro-to-NLP-pretrained/main/word_embedding_models/data/shakespeare.txt -O ./shakespeare.txt\r\n",
        "\r\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/aisutd/Intro-to-NLP-pretrained/main/word_embedding_models/data/warpeace.txt -O ./warpeace.txt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-NHO3Ce7W1"
      },
      "source": [
        "## Decode corpus into a list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S11yKT1wFlJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6d6fc5-6fa7-4ed6-f6e4-c682890f1f43"
      },
      "source": [
        "#### Can we make the loading corpus process easier? My plan is to define a function that can:  \r\n",
        "#### process: corpus--> parse --> sentence --> word_tokenize --> [ [word1,word2,word3...forming a sentence] , [word1,word2,word3...forming a sentence] ]\r\n",
        "#### it uses nltk library\r\n",
        "import nltk\r\n",
        "import tensorflow as tf\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.preprocessing.text import text_to_word_sequence\r\n",
        "\r\n",
        "## determine the path of training corpus\r\n",
        "corpus_path1 = 'warpeace.txt'\r\n",
        "corpus_path2 = 'shakespeare.txt'\r\n",
        "\r\n",
        "\r\n",
        "## Aim: make the following transformations: \r\n",
        "## corpus--> parse --> sentence --> word_tokenize --> [ [word1,word2,word3...forming a sentence] , [word1,word2,word3...forming a sentence] ]\r\n",
        "def load_corpus(corpus_path):\r\n",
        "  f = open(corpus_path,encoding='utf-8-sig')\r\n",
        "  content = f.read()\r\n",
        "  ## in the warpeace.txt, there are two newline(Enter) characters between paragraphes, \r\n",
        "  ## so use .split('\\n\\n') to split paragraphs\r\n",
        "  paragraphs = content.split('\\n\\n')\r\n",
        "  sentences = []\r\n",
        "\r\n",
        "  ## if you want to break a paragraph into sentences, could use:\r\n",
        "  ## tokenize.sent_tokenize(paragraph) \r\n",
        "  ## it will break a sentence (str) into \r\n",
        "  ## a list of strings, each string is a sentence\r\n",
        "  for i in paragraphs:\r\n",
        "    sentences.extend(nltk.tokenize.sent_tokenize(i))\r\n",
        "  \r\n",
        "  words = []\r\n",
        "  ## to break a sentence into words\r\n",
        "  ## could use text_to_word_sequence(i)\r\n",
        "  for i in sentences:\r\n",
        "    temp = text_to_word_sequence(i)\r\n",
        "    words.append(temp)\r\n",
        "  \r\n",
        "  return words\r\n",
        "\r\n",
        "corpus1 = load_corpus(corpus_path1)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTMBE6arh81H"
      },
      "source": [
        "## Visualize this corpus decoding process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0LWkEWKJqdM",
        "outputId": "e07176bd-d376-4b3d-b8c6-7b3b71901ed3"
      },
      "source": [
        "## load_corpus function test \r\n",
        "f = open(corpus_path1,encoding='utf-8-sig')\r\n",
        "content = f.read()\r\n",
        "paragraphs = content.split('\\n\\n')\r\n",
        "print('origional paragraph:')\r\n",
        "print(paragraphs[0])\r\n",
        "print()\r\n",
        "\r\n",
        "group_of_sentence = nltk.tokenize.sent_tokenize(paragraphs[0])\r\n",
        "print(\"after sent_tokenize(paragraph)\")\r\n",
        "print(group_of_sentence)\r\n",
        "print()\r\n",
        "\r\n",
        "group_of_words = text_to_word_sequence(group_of_sentence[0])\r\n",
        "print(\"after text_to_word_sequence(sentence)\")\r\n",
        "print(group_of_words)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "origional paragraph:\n",
            "\"Well, Prince, so Genoa and Lucca are now just family estates of the\n",
            "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
            "if you still try to defend the infamies and horrors perpetrated by that\n",
            "Antichrist--I really believe he is Antichrist--I will have nothing more\n",
            "to do with you and you are no longer my friend, no longer my 'faithful\n",
            "slave,' as you call yourself! But how do you do? I see I have frightened\n",
            "you--sit down and tell me all the news.\"\n",
            "\n",
            "after sent_tokenize(paragraph)\n",
            "['\"Well, Prince, so Genoa and Lucca are now just family estates of the\\nBuonapartes.', \"But I warn you, if you don't tell me that this means war,\\nif you still try to defend the infamies and horrors perpetrated by that\\nAntichrist--I really believe he is Antichrist--I will have nothing more\\nto do with you and you are no longer my friend, no longer my 'faithful\\nslave,' as you call yourself!\", 'But how do you do?', 'I see I have frightened\\nyou--sit down and tell me all the news.\"']\n",
            "\n",
            "after word_tokenize(sentence)\n",
            "['well', 'prince', 'so', 'genoa', 'and', 'lucca', 'are', 'now', 'just', 'family', 'estates', 'of', 'the', 'buonapartes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNkWhqswuJD"
      },
      "source": [
        "## Give each unique word an index\r\n",
        "so later, we can use index to present any word \r\n",
        "\r\n",
        "For example: ['before', 'we', 'proceed', 'any','further', 'hear', 'me', 'speak'] --> [138, 36, 982, 144, 673, 125, 16, 106]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn9QEgJmIDcy"
      },
      "source": [
        "## use tokenizer to give each word in corpus a unique index\r\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\r\n",
        "## tokenizer.fit_on_texts(corpus)\r\n",
        "tokenizer.fit_on_texts(corpus1)\r\n",
        "w2id = tokenizer.word_index\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScOeeM_6JO29",
        "outputId": "1b3c4d57-0229-40d3-c3f4-afe2364caeeb"
      },
      "source": [
        "len(w2id)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQFrQ6DVySRm"
      },
      "source": [
        "## Generating embeddings training data\r\n",
        "  - The inputs (features) are surrounding words \r\n",
        "  - The output (predictions) is the target word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BH10tUx0urk"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pissk0456R68"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "window_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVYlisjiJWu8"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "def generate_pairs(window_size, corpus):\r\n",
        "  ## line_num: how many lines to use as training data from our corpus\r\n",
        "  line_num = 600\r\n",
        "  \r\n",
        "  ## randomly choose lines from corpus\r\n",
        "  random.shuffle(corpus)\r\n",
        "  corpus = corpus[:line_num]\r\n",
        "  \r\n",
        "  X = []\r\n",
        "  y = []\r\n",
        "\r\n",
        "  for words in corpus:\r\n",
        "    start = 0\r\n",
        "    while start + window_size * 2 < len(words):\r\n",
        "      end = start + window_size * 2\r\n",
        "      tar_i = start + window_size\r\n",
        "      x = [\" \".join(words[start:tar_i] + words[tar_i+1:end+1])]\r\n",
        "      label = words[tar_i]\r\n",
        "      # print(words)\r\n",
        "      # print(x, \"--->\", label)\r\n",
        "      start += 1\r\n",
        "      X.append(tokenizer.texts_to_sequences(x)[0])\r\n",
        "      y.append(to_categorical( tokenizer.word_index[label], len(tokenizer.word_index) + 1))\r\n",
        "\r\n",
        "  return tf.convert_to_tensor(X) , tf.convert_to_tensor(y)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNYKyw5z1Rjl"
      },
      "source": [
        "X_train, y_train = generate_pairs(window_size, corpus1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0VxGFVTY0OD"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esHmdMRgfgkt"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OyslBFKyq4q"
      },
      "source": [
        "## CBOW model\r\n",
        "1. defind model structure: \r\n",
        "- defining each layer\r\n",
        "- defining how input data flows from layer to layer\r\n",
        "- defining loss function\r\n",
        "2. train model using our training data\r\n",
        "3. upgrade weight\r\n",
        "\r\n",
        "Model configuration: \r\n",
        "One Embedding (hidden) layer the size of the output embedding vectors, \r\n",
        "One lambda layer to calculate the mean over all embedding vectors (target vectors and context vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvRw8Hva5bd7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\r\n",
        "from tensorflow.keras.backend import mean \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl8WczPIPR3f",
        "outputId": "923d5e36-c32a-4a8e-dd5c-c8512bd65c63"
      },
      "source": [
        "X_train[0], y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([  3,  19, 932,  19], dtype=int32)>,\n",
              " <tf.Tensor: shape=(18284,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbw3UJV_5lpN"
      },
      "source": [
        "#### want to explain each layer in detail? \r\n",
        "#### explain the loss function?\r\n",
        "embedding_size = 128\r\n",
        "cbow = Sequential()\r\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=window_size*2))\r\n",
        "cbow.add(Lambda(lambda x: mean(x, axis=1), output_shape=(embedding_size,) ))\r\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\r\n",
        "\r\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "cbow.fit(X_train, y_train, epochs=40, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjHxO7PE6d6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b713a873-0030-4950-ed18-f3d6b79858bc"
      },
      "source": [
        "## model cbow have finished training\r\n",
        "\r\n",
        "cbow.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 128)            2340352   \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 18284)             2358636   \n",
            "=================================================================\n",
            "Total params: 4,698,988\n",
            "Trainable params: 4,698,988\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1RCORcy7Vf"
      },
      "source": [
        "*Examining the trained model and check embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj2y1LLvvZTF"
      },
      "source": [
        "## model.get_weights() will get weight of all layers, here, we need the first layer\r\n",
        "weights = cbow.get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B8CrmtrpQNk"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "13bBZ1yWwTDt",
        "outputId": "ffb33fa9-86eb-40e1-d2c5-c4765837f51b"
      },
      "source": [
        "## visualize the embedding of each word\r\n",
        "import pandas as pd\r\n",
        "weights = cbow.get_weights()[0]\r\n",
        "weights = weights[1:]\r\n",
        "print(weights.shape)\r\n",
        "\r\n",
        "pd.DataFrame(weights, index=list(tokenizer.index_word.values())).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18283, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;OOV&gt;</th>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.041296</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.020388</td>\n",
              "      <td>0.028730</td>\n",
              "      <td>-0.028484</td>\n",
              "      <td>0.030016</td>\n",
              "      <td>-0.041115</td>\n",
              "      <td>0.026272</td>\n",
              "      <td>-0.013379</td>\n",
              "      <td>-0.027312</td>\n",
              "      <td>-0.039701</td>\n",
              "      <td>-0.032826</td>\n",
              "      <td>0.034481</td>\n",
              "      <td>-0.031482</td>\n",
              "      <td>0.022481</td>\n",
              "      <td>-0.016323</td>\n",
              "      <td>-0.002279</td>\n",
              "      <td>0.046065</td>\n",
              "      <td>-0.023169</td>\n",
              "      <td>0.011711</td>\n",
              "      <td>-0.027386</td>\n",
              "      <td>0.017944</td>\n",
              "      <td>-0.009765</td>\n",
              "      <td>-0.047634</td>\n",
              "      <td>0.031039</td>\n",
              "      <td>-0.029459</td>\n",
              "      <td>-0.035880</td>\n",
              "      <td>-0.039169</td>\n",
              "      <td>-0.016874</td>\n",
              "      <td>0.018874</td>\n",
              "      <td>-0.047782</td>\n",
              "      <td>0.040012</td>\n",
              "      <td>0.037530</td>\n",
              "      <td>-0.025602</td>\n",
              "      <td>-0.039447</td>\n",
              "      <td>0.012189</td>\n",
              "      <td>0.029454</td>\n",
              "      <td>0.033225</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.043208</td>\n",
              "      <td>-0.017689</td>\n",
              "      <td>-0.040157</td>\n",
              "      <td>-0.032527</td>\n",
              "      <td>-0.045790</td>\n",
              "      <td>0.038097</td>\n",
              "      <td>0.006811</td>\n",
              "      <td>0.021862</td>\n",
              "      <td>0.034570</td>\n",
              "      <td>0.039704</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>-0.023852</td>\n",
              "      <td>-0.015273</td>\n",
              "      <td>-0.018412</td>\n",
              "      <td>-0.030702</td>\n",
              "      <td>-0.017098</td>\n",
              "      <td>0.006126</td>\n",
              "      <td>-0.048408</td>\n",
              "      <td>0.047038</td>\n",
              "      <td>-0.018962</td>\n",
              "      <td>-0.044467</td>\n",
              "      <td>-0.017508</td>\n",
              "      <td>0.049819</td>\n",
              "      <td>0.045960</td>\n",
              "      <td>0.048172</td>\n",
              "      <td>-0.005494</td>\n",
              "      <td>-0.008210</td>\n",
              "      <td>0.005549</td>\n",
              "      <td>-0.022838</td>\n",
              "      <td>-0.021250</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.024100</td>\n",
              "      <td>-0.043886</td>\n",
              "      <td>0.009294</td>\n",
              "      <td>0.031871</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.035638</td>\n",
              "      <td>0.014822</td>\n",
              "      <td>0.003821</td>\n",
              "      <td>-0.000264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.257736</td>\n",
              "      <td>0.293641</td>\n",
              "      <td>0.692132</td>\n",
              "      <td>0.046530</td>\n",
              "      <td>-0.018028</td>\n",
              "      <td>0.309569</td>\n",
              "      <td>0.498804</td>\n",
              "      <td>-0.751291</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>-1.161633</td>\n",
              "      <td>1.399045</td>\n",
              "      <td>0.901222</td>\n",
              "      <td>1.212245</td>\n",
              "      <td>0.337493</td>\n",
              "      <td>-1.077747</td>\n",
              "      <td>0.393239</td>\n",
              "      <td>0.868459</td>\n",
              "      <td>-1.218264</td>\n",
              "      <td>-0.702668</td>\n",
              "      <td>-0.508949</td>\n",
              "      <td>1.379773</td>\n",
              "      <td>0.684562</td>\n",
              "      <td>-0.478001</td>\n",
              "      <td>1.934656</td>\n",
              "      <td>-0.640410</td>\n",
              "      <td>-1.368634</td>\n",
              "      <td>0.648364</td>\n",
              "      <td>0.432985</td>\n",
              "      <td>0.120155</td>\n",
              "      <td>-0.769256</td>\n",
              "      <td>-0.937215</td>\n",
              "      <td>0.829384</td>\n",
              "      <td>0.139868</td>\n",
              "      <td>-2.333879</td>\n",
              "      <td>0.027170</td>\n",
              "      <td>1.629406</td>\n",
              "      <td>-1.059711</td>\n",
              "      <td>0.312353</td>\n",
              "      <td>-2.309788</td>\n",
              "      <td>0.516218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.189327</td>\n",
              "      <td>-1.208838</td>\n",
              "      <td>-0.672895</td>\n",
              "      <td>-0.530488</td>\n",
              "      <td>0.119249</td>\n",
              "      <td>0.234300</td>\n",
              "      <td>0.544226</td>\n",
              "      <td>0.112984</td>\n",
              "      <td>-0.339929</td>\n",
              "      <td>0.416541</td>\n",
              "      <td>1.343238</td>\n",
              "      <td>0.105336</td>\n",
              "      <td>0.858705</td>\n",
              "      <td>-0.116132</td>\n",
              "      <td>0.280401</td>\n",
              "      <td>0.778585</td>\n",
              "      <td>1.128028</td>\n",
              "      <td>-1.609462</td>\n",
              "      <td>0.737383</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>1.264146</td>\n",
              "      <td>-0.194314</td>\n",
              "      <td>0.841291</td>\n",
              "      <td>-0.112459</td>\n",
              "      <td>-1.020169</td>\n",
              "      <td>-0.341742</td>\n",
              "      <td>0.430846</td>\n",
              "      <td>-0.931552</td>\n",
              "      <td>-0.778793</td>\n",
              "      <td>-0.806238</td>\n",
              "      <td>0.835324</td>\n",
              "      <td>-0.590898</td>\n",
              "      <td>0.860698</td>\n",
              "      <td>-0.622212</td>\n",
              "      <td>0.129965</td>\n",
              "      <td>-0.928830</td>\n",
              "      <td>-1.270448</td>\n",
              "      <td>-0.074886</td>\n",
              "      <td>-0.900748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.845702</td>\n",
              "      <td>-1.450420</td>\n",
              "      <td>-1.221642</td>\n",
              "      <td>1.125943</td>\n",
              "      <td>-0.816225</td>\n",
              "      <td>-0.062149</td>\n",
              "      <td>1.751172</td>\n",
              "      <td>0.671192</td>\n",
              "      <td>1.871115</td>\n",
              "      <td>0.823485</td>\n",
              "      <td>-1.237643</td>\n",
              "      <td>-1.239474</td>\n",
              "      <td>0.622732</td>\n",
              "      <td>1.087603</td>\n",
              "      <td>0.440104</td>\n",
              "      <td>0.901239</td>\n",
              "      <td>1.464554</td>\n",
              "      <td>1.128371</td>\n",
              "      <td>-1.238841</td>\n",
              "      <td>-0.068189</td>\n",
              "      <td>-0.972308</td>\n",
              "      <td>0.227093</td>\n",
              "      <td>0.983175</td>\n",
              "      <td>-0.929632</td>\n",
              "      <td>0.615596</td>\n",
              "      <td>0.698862</td>\n",
              "      <td>0.016288</td>\n",
              "      <td>-1.057213</td>\n",
              "      <td>-0.049349</td>\n",
              "      <td>0.829984</td>\n",
              "      <td>-0.341298</td>\n",
              "      <td>-1.095851</td>\n",
              "      <td>0.847433</td>\n",
              "      <td>1.163382</td>\n",
              "      <td>-0.398383</td>\n",
              "      <td>0.195129</td>\n",
              "      <td>0.889611</td>\n",
              "      <td>-0.720562</td>\n",
              "      <td>0.687627</td>\n",
              "      <td>-0.299999</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.168867</td>\n",
              "      <td>0.306315</td>\n",
              "      <td>0.714580</td>\n",
              "      <td>-0.910497</td>\n",
              "      <td>-0.519435</td>\n",
              "      <td>0.837615</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.741754</td>\n",
              "      <td>0.701022</td>\n",
              "      <td>0.224566</td>\n",
              "      <td>0.198512</td>\n",
              "      <td>-0.007896</td>\n",
              "      <td>-0.102169</td>\n",
              "      <td>0.885575</td>\n",
              "      <td>-1.208625</td>\n",
              "      <td>0.508875</td>\n",
              "      <td>-0.263460</td>\n",
              "      <td>0.271034</td>\n",
              "      <td>0.137292</td>\n",
              "      <td>-1.390332</td>\n",
              "      <td>-0.745570</td>\n",
              "      <td>-0.889107</td>\n",
              "      <td>0.480599</td>\n",
              "      <td>1.191729</td>\n",
              "      <td>-1.122144</td>\n",
              "      <td>0.486226</td>\n",
              "      <td>0.248608</td>\n",
              "      <td>-1.277798</td>\n",
              "      <td>-1.054769</td>\n",
              "      <td>-0.353150</td>\n",
              "      <td>-0.505930</td>\n",
              "      <td>0.093927</td>\n",
              "      <td>0.264163</td>\n",
              "      <td>-0.844490</td>\n",
              "      <td>0.429597</td>\n",
              "      <td>-1.137976</td>\n",
              "      <td>-0.800089</td>\n",
              "      <td>-0.977990</td>\n",
              "      <td>-1.547301</td>\n",
              "      <td>-0.467962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.350556</td>\n",
              "      <td>-0.079439</td>\n",
              "      <td>1.021105</td>\n",
              "      <td>1.552466</td>\n",
              "      <td>-0.284418</td>\n",
              "      <td>0.761033</td>\n",
              "      <td>0.095495</td>\n",
              "      <td>2.128414</td>\n",
              "      <td>-0.256369</td>\n",
              "      <td>-1.577548</td>\n",
              "      <td>-1.561545</td>\n",
              "      <td>-0.660781</td>\n",
              "      <td>-1.552087</td>\n",
              "      <td>-1.190119</td>\n",
              "      <td>-0.128892</td>\n",
              "      <td>-1.448903</td>\n",
              "      <td>0.592595</td>\n",
              "      <td>0.811291</td>\n",
              "      <td>1.133071</td>\n",
              "      <td>0.965412</td>\n",
              "      <td>0.608937</td>\n",
              "      <td>-0.449273</td>\n",
              "      <td>0.116730</td>\n",
              "      <td>-0.416856</td>\n",
              "      <td>-0.273741</td>\n",
              "      <td>0.901183</td>\n",
              "      <td>-1.291070</td>\n",
              "      <td>1.556628</td>\n",
              "      <td>-0.707583</td>\n",
              "      <td>0.120663</td>\n",
              "      <td>0.675146</td>\n",
              "      <td>0.430638</td>\n",
              "      <td>1.022249</td>\n",
              "      <td>-0.113210</td>\n",
              "      <td>0.235896</td>\n",
              "      <td>0.674733</td>\n",
              "      <td>-0.305709</td>\n",
              "      <td>-0.530438</td>\n",
              "      <td>-0.042419</td>\n",
              "      <td>-0.406645</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.286296</td>\n",
              "      <td>0.845473</td>\n",
              "      <td>-1.459607</td>\n",
              "      <td>-0.459425</td>\n",
              "      <td>0.932763</td>\n",
              "      <td>-0.995375</td>\n",
              "      <td>-1.274460</td>\n",
              "      <td>-0.422914</td>\n",
              "      <td>0.245985</td>\n",
              "      <td>0.614344</td>\n",
              "      <td>-1.274514</td>\n",
              "      <td>-1.010206</td>\n",
              "      <td>0.146373</td>\n",
              "      <td>-0.660546</td>\n",
              "      <td>0.688233</td>\n",
              "      <td>1.944947</td>\n",
              "      <td>1.123513</td>\n",
              "      <td>-1.022935</td>\n",
              "      <td>0.458373</td>\n",
              "      <td>1.011283</td>\n",
              "      <td>0.599070</td>\n",
              "      <td>-0.792976</td>\n",
              "      <td>-0.330787</td>\n",
              "      <td>-0.190308</td>\n",
              "      <td>-0.089556</td>\n",
              "      <td>0.960643</td>\n",
              "      <td>0.250992</td>\n",
              "      <td>0.778522</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>-1.122532</td>\n",
              "      <td>-0.370896</td>\n",
              "      <td>0.575396</td>\n",
              "      <td>-0.447636</td>\n",
              "      <td>-0.706243</td>\n",
              "      <td>-0.192837</td>\n",
              "      <td>0.605254</td>\n",
              "      <td>-1.204168</td>\n",
              "      <td>0.931996</td>\n",
              "      <td>0.712831</td>\n",
              "      <td>1.176523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.243352</td>\n",
              "      <td>0.871903</td>\n",
              "      <td>0.069889</td>\n",
              "      <td>-1.742707</td>\n",
              "      <td>0.087215</td>\n",
              "      <td>0.499025</td>\n",
              "      <td>0.678177</td>\n",
              "      <td>-0.957983</td>\n",
              "      <td>-1.232533</td>\n",
              "      <td>1.333122</td>\n",
              "      <td>-0.719711</td>\n",
              "      <td>0.525157</td>\n",
              "      <td>0.336917</td>\n",
              "      <td>-0.697553</td>\n",
              "      <td>-1.153169</td>\n",
              "      <td>-1.026292</td>\n",
              "      <td>1.221626</td>\n",
              "      <td>0.716688</td>\n",
              "      <td>1.198959</td>\n",
              "      <td>0.086602</td>\n",
              "      <td>-0.959524</td>\n",
              "      <td>0.023762</td>\n",
              "      <td>-1.279881</td>\n",
              "      <td>-0.556938</td>\n",
              "      <td>-0.238896</td>\n",
              "      <td>0.864581</td>\n",
              "      <td>1.226758</td>\n",
              "      <td>-1.348667</td>\n",
              "      <td>1.535303</td>\n",
              "      <td>-1.358394</td>\n",
              "      <td>-1.285847</td>\n",
              "      <td>-0.753889</td>\n",
              "      <td>0.413269</td>\n",
              "      <td>0.187787</td>\n",
              "      <td>-0.113171</td>\n",
              "      <td>0.127241</td>\n",
              "      <td>-0.342945</td>\n",
              "      <td>0.794270</td>\n",
              "      <td>-0.415729</td>\n",
              "      <td>1.460319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093836</td>\n",
              "      <td>1.296466</td>\n",
              "      <td>-0.998680</td>\n",
              "      <td>-0.710773</td>\n",
              "      <td>1.798364</td>\n",
              "      <td>-0.409993</td>\n",
              "      <td>-1.392506</td>\n",
              "      <td>-1.459902</td>\n",
              "      <td>-0.468733</td>\n",
              "      <td>-0.103662</td>\n",
              "      <td>1.523205</td>\n",
              "      <td>-0.101150</td>\n",
              "      <td>1.139579</td>\n",
              "      <td>-0.920750</td>\n",
              "      <td>0.441831</td>\n",
              "      <td>-1.018171</td>\n",
              "      <td>-0.345307</td>\n",
              "      <td>1.036974</td>\n",
              "      <td>-0.437520</td>\n",
              "      <td>0.448377</td>\n",
              "      <td>-1.541105</td>\n",
              "      <td>-0.638920</td>\n",
              "      <td>-1.151975</td>\n",
              "      <td>-0.591554</td>\n",
              "      <td>-0.536341</td>\n",
              "      <td>0.482482</td>\n",
              "      <td>0.723538</td>\n",
              "      <td>-0.683353</td>\n",
              "      <td>-0.503783</td>\n",
              "      <td>1.011101</td>\n",
              "      <td>0.928226</td>\n",
              "      <td>-0.785141</td>\n",
              "      <td>-1.359758</td>\n",
              "      <td>-0.336939</td>\n",
              "      <td>0.810617</td>\n",
              "      <td>-0.215666</td>\n",
              "      <td>1.002034</td>\n",
              "      <td>0.166432</td>\n",
              "      <td>-0.908564</td>\n",
              "      <td>1.021019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2    ...       125       126       127\n",
              "<OOV>  0.000404  0.041296  0.001973  ...  0.014822  0.003821 -0.000264\n",
              "the   -0.257736  0.293641  0.692132  ... -1.270448 -0.074886 -0.900748\n",
              "and   -0.845702 -1.450420 -1.221642  ... -0.977990 -1.547301 -0.467962\n",
              "to     0.350556 -0.079439  1.021105  ...  0.931996  0.712831  1.176523\n",
              "of     0.243352  0.871903  0.069889  ...  0.166432 -0.908564  1.021019\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpqA0L35waFS",
        "outputId": "51baa979-f122-47de-ba3e-1f4a6e5f9acd"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18283, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bntiu34MxVmY"
      },
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXWC45pinChV"
      },
      "source": [
        "#### might take 15 mins, while weights.shape is (18283, 128)\r\n",
        "## TSNE model is a way to visualize the high dementional embeddings \r\n",
        "## use TSNE to transfer high dementional embeddings into a 2 dementional graph, similar embeddings are closer\r\n",
        "tsne_model_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\r\n",
        "embeddings_2d = tsne_model_2d.fit_transform(weights)\r\n",
        "vocab_embeddings_2d = pd.DataFrame(embeddings_2d, index=list(tokenizer.index_word.values()),columns = ['x-axis','y-axis'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xNJCNZ3tEBG"
      },
      "source": [
        "#### I use some word to test our embeddings, add word if you wish\r\n",
        "target_word_list = ['cat','dog','animal','law','justice','judge','create','wood','tree','forest','love','bear','masters','friends','curse','enemies']\r\n",
        "temp_words = list(tokenizer.index_word.values())\r\n",
        "temp_embeddings = [ embeddings_2d[temp_words.index(x)] for x in target_word_list]\r\n",
        "vocab_embeddings_2d = pd.DataFrame(temp_embeddings, index=target_word_list,columns = ['x-axis','y-axis'])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDXDE5icobRa"
      },
      "source": [
        "## random_words_set_2d = vocab_embeddings_2d.sample(n=30, random_state=0)\r\n",
        "## random_words_set_2d\r\n",
        "random_words_set_2d = vocab_embeddings_2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ZyE_ZAolpdvm",
        "outputId": "779230e4-3d4d-4284-ec38-2b3d43f37ae7"
      },
      "source": [
        "#### training corpus is not large enough to show wonderful embeddings\r\n",
        "#### but the time complexity of this algrithm do not support a large corpus\r\n",
        "#### so how about use a pretrained embedding to show their similarity? like the GloVe code I previded\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "fig, ax = plt.subplots()\r\n",
        "random_words_set_2d.plot(0, 1, kind='scatter', ax=ax)\r\n",
        "\r\n",
        "for k, v in random_words_set_2d.iterrows():\r\n",
        "  ax.annotate(k, v)\r\n",
        "\r\n",
        "fig.canvas.draw()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}